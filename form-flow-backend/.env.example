# API Keys for LLM Integration
GOOGLE_API_KEY=your_google_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Optional: For production deployment
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Redis (for caching and rate limiting - REQUIRED for 1000+ users)
REDIS_URL=redis://default:password@host:port

# Optional: For enhanced speech services
DEEPGRAM_API_KEY=your_deepgram_api_key_here
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here

# Security
SECRET_KEY=your_super_secret_key_change_in_production

# =============================================================================
# SCALABILITY CONFIGURATION (for AWS t2.micro/t3.micro with 1GB RAM)
# =============================================================================

# Enable low memory mode for 1GB RAM servers (default: true)
LOW_MEMORY_MODE=true

# Number of Gunicorn workers (default: 1 in low memory mode)
# Recommended: 1 for t2.micro, 2 for t2.small, 3-4 for t2.medium+
GUNICORN_WORKERS=1

# Maximum concurrent browser contexts (default: 5)
# Each context uses ~50MB RAM. Recommended:
# - t2.micro (1GB):  3
# - t2.small (2GB):  5
# - t2.medium (4GB): 8
BROWSER_POOL_MAX_CONTEXTS=3

# =============================================================================
# SMART QUESTION ENGINE (Reduces ~150 fields to ~30 intelligent questions)
# =============================================================================

# Enable smart field grouping for web clients (default: false)
SMART_GROUPING_ENABLED=false

# Minimum fill ratio to consider a group complete (default: 0.7 = 70%)
SMART_GROUPING_MIN_FILL_RATIO=0.7