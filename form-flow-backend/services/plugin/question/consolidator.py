"""
LLM Question Consolidation Service

Uses LLM to generate natural, conversational questions from field batches.
Features:
- Context-aware question generation
- Multi-field consolidation
- Retry with exponential backoff
- Cost tracking per plugin

Zero redundancy:
- Reuses LLM client from existing infrastructure
- Single prompt template
"""

import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

from services.plugin.question.optimizer import OptimizedQuestion
from services.plugin.question.cost_tracker import CostTracker, get_cost_tracker
from utils.circuit_breaker import resilient_call
from utils.logging import get_logger

logger = get_logger(__name__)


# Token cost estimates (per 1M tokens)
# These are approximate and should be updated based on actual pricing
TOKEN_COSTS = {
    "gemini-2.5-flash-lite": {"input": 0.075, "output": 0.30},
    "gemini-2.0-flash": {"input": 0.10, "output": 0.40},
    "gpt-4o-mini": {"input": 0.15, "output": 0.60},
}


@dataclass
class ConsolidatedQuestion:
    """A question generated by LLM consolidation."""
    original_fields: List[Dict[str, Any]]
    consolidated_text: str
    field_names: List[str]
    tokens_used: int
    estimated_cost: float


class QuestionConsolidator:
    """
    LLM-powered question consolidation.
    
    Takes optimized field batches and generates natural,
    conversational questions using an LLM.
    
    Usage:
        consolidator = QuestionConsolidator(llm_client)
        questions = await consolidator.consolidate_batch(plugin_id, batch)
    """
    
    # Prompt template for question consolidation
    CONSOLIDATION_PROMPT = """You are helping create a natural, conversational question to collect user information.

Given these data fields to collect:
{field_descriptions}

Generate a single, natural question that asks for all this information in a friendly, conversational way.

Rules:
1. Be concise - one sentence if possible
2. Use natural language, not technical terms
3. Make it sound like a helpful assistant, not a form
4. If fields are related, group them naturally
5. Ensure all fields are covered

Return ONLY the question text, nothing else."""

    def __init__(self, llm_client=None, model_name: str = "gemini-2.5-flash-lite"):
        """
        Initialize consolidator.
        
        Args:
            llm_client: LangChain LLM client (lazy loaded if None)
            model_name: Model name for cost tracking
        """
        self._llm_client = llm_client
        self._model_name = model_name
        self._cost_tracker = get_cost_tracker()
    
    async def _get_llm_client(self):
        """Lazy load LLM client."""
        if self._llm_client is None:
            from langchain_google_genai import ChatGoogleGenerativeAI
            from config.settings import settings
            
            self._llm_client = ChatGoogleGenerativeAI(
                model=self._model_name,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0.3,  # Low temp for consistent questions
            )
        return self._llm_client
    
    def _build_prompt(self, fields: List[Dict[str, Any]]) -> str:
        """Build consolidation prompt from fields."""
        field_descriptions = []
        
        for field in fields:
            name = field.get('column_name', 'unknown')
            desc = field.get('question_text', name)
            required = "required" if field.get('is_required') else "optional"
            
            field_descriptions.append(f"- {name}: {desc} ({required})")
        
        return self.CONSOLIDATION_PROMPT.format(
            field_descriptions="\n".join(field_descriptions)
        )
    
    def _estimate_tokens(self, prompt: str, response: str) -> int:
        """Estimate token count (rough approximation)."""
        # Rough estimate: 1 token â‰ˆ 4 characters
        total_chars = len(prompt) + len(response)
        return total_chars // 4
    
    def _estimate_cost(self, tokens: int) -> float:
        """Estimate cost based on tokens."""
        pricing = TOKEN_COSTS.get(self._model_name, TOKEN_COSTS["gemini-2.5-flash-lite"])
        # Assume 70% input, 30% output
        input_tokens = int(tokens * 0.7)
        output_tokens = int(tokens * 0.3)
        
        cost = (input_tokens / 1_000_000 * pricing["input"] +
                output_tokens / 1_000_000 * pricing["output"])
        return round(cost, 6)
    
    async def consolidate_question(
        self,
        plugin_id: int,
        fields: List[Dict[str, Any]]
    ) -> ConsolidatedQuestion:
        """
        Consolidate multiple fields into a natural question using LLM.
        
        Args:
            plugin_id: Plugin ID for cost tracking
            fields: List of plugin fields
            
        Returns:
            ConsolidatedQuestion with generated text
        """
        prompt = self._build_prompt(fields)
        
        # Call LLM with retry
        llm = await self._get_llm_client()
        
        from langchain_core.messages import HumanMessage
        
        response = await resilient_call(
            llm.ainvoke,
            [HumanMessage(content=prompt)],
            max_retries=3,
            circuit_name=f"llm_consolidate_{plugin_id}"
        )
        
        response_text = response.content.strip()
        
        # Track usage
        tokens = self._estimate_tokens(prompt, response_text)
        cost = self._estimate_cost(tokens)
        
        await self._cost_tracker.track_usage(
            plugin_id=plugin_id,
            operation="question_consolidation",
            tokens=tokens,
            estimated_cost=cost,
            model=self._model_name
        )
        
        logger.info(f"Consolidated {len(fields)} fields for plugin {plugin_id}: {tokens} tokens, ${cost}")
        
        return ConsolidatedQuestion(
            original_fields=fields,
            consolidated_text=response_text,
            field_names=[f.get('column_name', '') for f in fields],
            tokens_used=tokens,
            estimated_cost=cost
        )
    
    async def consolidate_batch(
        self,
        plugin_id: int,
        questions: List[OptimizedQuestion],
        min_fields_for_llm: int = 2
    ) -> List[OptimizedQuestion]:
        """
        Consolidate all questions in a batch.
        
        Uses LLM for multi-field questions, keeps simple questions as-is.
        
        Args:
            plugin_id: Plugin ID for cost tracking
            questions: List of OptimizedQuestion objects
            min_fields_for_llm: Minimum fields to trigger LLM consolidation
            
        Returns:
            List of OptimizedQuestion with consolidated text
        """
        consolidated = []
        
        for q in questions:
            if len(q.fields) >= min_fields_for_llm:
                # Use LLM for multi-field questions
                result = await self.consolidate_question(plugin_id, q.fields)
                consolidated.append(OptimizedQuestion(
                    fields=q.fields,
                    question_text=result.consolidated_text,
                    field_names=q.field_names,
                    group=q.group,
                    complexity=q.complexity
                ))
            else:
                # Keep original question text
                consolidated.append(q)
        
        return consolidated


# Singleton instance
_consolidator: Optional[QuestionConsolidator] = None


def get_question_consolidator(
    llm_client=None,
    model_name: str = "gemini-2.5-flash-lite"
) -> QuestionConsolidator:
    """Get singleton question consolidator."""
    global _consolidator
    if _consolidator is None:
        _consolidator = QuestionConsolidator(llm_client, model_name)
    return _consolidator
